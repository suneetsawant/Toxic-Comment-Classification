{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic Comments.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "7HezEbvZpiNN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/suneetsawant/nlp/blob/master/Toxic_Comments.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "FM5Gl0DSfng8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import all the libraries "
      ]
    },
    {
      "metadata": {
        "id": "tsVXLNURUwpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18631182-64c1-4619-fc41-a44a2ee9727a"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import os,shutil\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import CuDNNGRU,CuDNNLSTM,Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.models import Model,Sequential\n",
        "import seaborn as sns\n",
        "from keras.models import model_from_json\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Bidirectional\n",
        "import pickle\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "mmcGd363fOz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set up pydrive for uploading and downloading files (model and weights) \n",
        "\n",
        "The ***Drive()*** class gives method ***fileaction()***  to either upload or download a list of files \n",
        "\n",
        "### Usage :  \n",
        "      \n",
        "     To upload files named f1,f2  \n",
        "        Drive().fileaction([f1,f2],'up')\n",
        "     Similarly to download  \n",
        "        Drive().fileaction([f1,f2],'down')"
      ]
    },
    {
      "metadata": {
        "id": "51iW2yFKdA2c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "class Drive():   \n",
        "  def __init__(self) :  \n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    self.drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "  def fileaction(self,files,op='up') : \n",
        "    file_list = self.drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "    for filename in files:\n",
        "      flag = 0 \n",
        "\n",
        "      for file1 in file_list:\n",
        "        if (file1['title']) == filename :\n",
        "            if (op == 'up'and flag==0) : \n",
        "                file1.Delete()\n",
        "                self.upload(filename)\n",
        "                flag = 1\n",
        "\n",
        "            elif (op == 'down') : \n",
        "                self.download(filename,file1) \n",
        "\n",
        "      if(op=='up' and flag==0): \n",
        "            self.upload(filename)\n",
        "            flag = 1\n",
        "\n",
        "  def upload(self,filename) : \n",
        "      Uploadfile = self.drive.CreateFile({'title': filename})\n",
        "      Uploadfile.SetContentFile(filename)\n",
        "      Uploadfile.Upload()\n",
        "      print(\"Saved '{}' to Drive\".format(filename))\n",
        "\n",
        "  def download(self,filename,file1): \n",
        "      downloaded = self.drive.CreateFile({'id':file1['id']})\n",
        "      downloaded.GetContentFile(filename)\n",
        "      print(\"Downloaded '{}' from Drive\".format(filename))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSvANakCU-4o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Download the Dataset**"
      ]
    },
    {
      "metadata": {
        "id": "Xtxty6OrdSVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the dataset from [Toxic Comments](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). \n",
        "\n",
        "Keep  a copy of the dataset on your root of google drive account . \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "I60Q9A3-6CDx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09b2447d-d7a4-4179-a4b1-7968a798b0ef"
      },
      "cell_type": "code",
      "source": [
        "Drive().fileaction(['train.csv'],'down')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded 'train.csv' from Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jbcZrfAxelRe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the glove wordvectors "
      ]
    },
    {
      "metadata": {
        "id": "coq_wtDpmV4h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "86781e1c-f8b1-46c8-dcf3-b6b0197a7fff"
      },
      "cell_type": "code",
      "source": [
        "! wget nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -o glove.6B.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-18 06:20:23--  http://nlp.stanford.edu/data/glove.6B.zip\r\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2018-06-18 06:20:23--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  40.7MB/s    in 22s     \n",
            "\n",
            "2018-06-18 06:20:45 (37.8 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7HezEbvZpiNN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing "
      ]
    },
    {
      "metadata": {
        "id": "W5FBULF4Fjgc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  def prepareData(restore , shuffle,valid_ratio) :\n",
        "      \n",
        "      if restore==0 : \n",
        "        \n",
        "        dtrain = pd.read_csv('train.csv')\n",
        "        dftrain, dfval = train_test_split(dtrain, test_size= valid_ratio,shuffle=shuffle)\n",
        "        \n",
        "        dftrain.to_csv('dftrain.csv',index=False)\n",
        "        Drive().fileaction(['dftrain.csv'],'up')\n",
        "        \n",
        "        dfval.to_csv('dfval.csv',index = False)\n",
        "        Drive().fileaction(['dfval.csv'],'up')\n",
        "      \n",
        "      else :  \n",
        "        Drive().fileaction(['dftrain.csv'],'down')\n",
        "        dftrain = pd.read_csv('dftrain.csv') \n",
        "        \n",
        "        Drive().fileaction(['dfval.csv'],'down')\n",
        "        dfval = pd.read_csv('dfval.csv')\n",
        "         \n",
        "      classes = dftrain.columns[2:]\n",
        "      Xtrain = dftrain['comment_text'] \n",
        "      Ytrain = dftrain[classes].values \n",
        "\n",
        "      Xval = dfval['comment_text'] \n",
        "      Yval = dfval[classes].values \n",
        "\n",
        "      return Xtrain,Ytrain,Xval,Yval,classes\n",
        "    \n",
        "  def createTokens(restore,vocab_size, data) :\n",
        "      if (restore==0) : \n",
        "        tokenizer = Tokenizer(num_words = vocab_size)\n",
        "        tokenizer.fit_on_texts(data)\n",
        "        file = open('tokenizer.pkl','wb') \n",
        "        pickle.dump(tokenizer,file)\n",
        "        file.close()\n",
        "        Drive().fileaction(['tokenizer.pkl'],'up')\n",
        "      else : \n",
        "        Drive().fileaction(['tokenizer.pkl'],'down')\n",
        "        file = open('tokenizer.pkl','rb') \n",
        "        tokenizer = pickle.load(file)\n",
        "        file.close()\n",
        "      return tokenizer, len(tokenizer.word_index)\n",
        "  \n",
        "  def tokenToSequence(data,tokenzier,max_len_sentence): \n",
        "      data = tokenizer.texts_to_sequences(data)\n",
        "      data = pad_sequences(data,maxlen=max_len_sentence)\n",
        "      return data   \n",
        "    \n",
        "  def createEmbeddingIndex (path)  :\n",
        "      embeddings_index = {}\n",
        "      f = open( path)\n",
        "      for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "          coefs = np.asarray(values[1:], dtype='float32')\n",
        "          embeddings_index[word] = coefs\n",
        "      f.close()\n",
        "      return embeddings_index\n",
        "    \n",
        "  def EmbeddingMatrix(tokenizer, path , embedding_dim): \n",
        "      word_index = tokenizer.word_index\n",
        "      embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "      embeddings_index = createEmbeddingIndex (path)\n",
        "      for word, i in word_index.items():\n",
        "          embedding_vector = embeddings_index.get(word)\n",
        "          if embedding_vector is not None:\n",
        "              # words not found in embedding index will be all-zeros.\n",
        "              embedding_matrix[i] = embedding_vector\n",
        "      return embedding_matrix       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4onxT1sZ9-Nz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define the Model "
      ]
    },
    {
      "metadata": {
        "id": "gGZL17qZ99b0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  def Model(restore,loss,optimizer,vocab_size, embedding_matrix, max_len_sentence): \n",
        "      if(restore): return restoreModel(loss,optimizer)\n",
        "      else : return newModel(loss,optimizer,vocab_size, embedding_matrix,max_len_sentence)\n",
        " \n",
        "\n",
        "  def newModel(loss,optimizer, vocab_size, embedding_matrix, max_len_sentence) : \n",
        "      model = Sequential()\n",
        "      #model.add(Embedding(vocab_size,256,input_length=max_len_sentence,name='Embedding'))\n",
        "      model.add(Embedding( input_dim = vocab_size+1,\n",
        "                            output_dim = embedding_matrix.shape[1],\n",
        "                            weights = [embedding_matrix],\n",
        "                            trainable = False,\n",
        "                            name='Embedding'))\n",
        "      model.add(CuDNNGRU(75,name =\"lstm\"))\n",
        "      \n",
        "      model.add(Dense(100,activation='relu',name='Dense1'))\n",
        "      model.add(Dropout(0.2,name='Dropout1')) \n",
        "      \n",
        "      model.add(Dense(6,activation='sigmoid',name='Dense2'))\n",
        "      model.compile (loss=loss,\n",
        "                        optimizer=optimizer)\n",
        "      return model \n",
        "  \n",
        "  def restoreModel(loss,optimizer): \n",
        "      \n",
        "      Drive().fileaction(['model.json','weights.hdf5'],'down') \n",
        "      with open('model.json', 'r') as json_file:\n",
        "          loaded_model_json = json_file.read()\n",
        "      model = model_from_json(loaded_model_json)\n",
        "      model.compile (loss=loss,\n",
        "                        optimizer=optimizer)\n",
        "      # load weights into new model\n",
        "      model.load_weights(\"weights.hdf5\")\n",
        "      print(\"Loaded saved model\")\n",
        "      return model\n",
        "    \n",
        "  def buildModel(X,Y,epochs,batchsize,loss,optimizer,restore,vocab_size, embedding_matrix,max_len_sentence): \n",
        "    \n",
        "      model = Model(restore,loss,optimizer,vocab_size, embedding_matrix,max_len_sentence)      \n",
        "      filepath = \"weights.hdf5\"\n",
        "      checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=1, \n",
        "                                   save_best_only=True, mode='auto')\n",
        "      callbacks_list = [checkpoint]\n",
        "\n",
        "      model.fit(X,Y, batch_size=batch_size, epochs=epochs,callbacks=callbacks_list,verbose=1)\n",
        "\n",
        "      model_json = model.to_json()\n",
        "      with open(\"model.json\", \"w\") as json_file:\n",
        "          json_file.write(model_json) \n",
        "\n",
        "      Drive().fileaction(['model.json','weights.hdf5'],'up') \n",
        "      return model\n",
        "    \n",
        "  def trainModel(Xtrain,Ytrain,Xval,Yval,epochs,batchsize,loss,optimizer,restore,vocab_size, embedding_matrix,max_len_sentence): \n",
        "       \n",
        "      if(restore==1) : flag = 1 \n",
        "      else: flag = 0    \n",
        "\n",
        "      for i in range(epochs) :\n",
        "        \n",
        "        if(flag==0 and i>0): flag = 1\n",
        "        model = buildModel(Xtrain,Ytrain,1,batchsize,loss,optimizer,flag,vocab_size, embedding_matrix,max_len_sentence)\n",
        "        trainScore = evaluateModel(Xtrain,Ytrain,model) \n",
        "        validScore = evaluateModel(Xval,Yval,model)\n",
        "        print('Training Score : {} - Validation Score:{}'.format(trainScore,validScore))\n",
        "      \n",
        "      model.summary()\n",
        "      print('Model is evaluated on metric ROC AUC')\n",
        "\n",
        "      return model\n",
        "    \n",
        "  def evaluateModel(X,Y,model): \n",
        "      preds = model.predict(X,verbose=1)\n",
        "      return roc_auc_score (Y,preds)\n",
        "    \n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XRPS2qvd9rvI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Training Model\n",
        "\n",
        "Keep **restore** = 0 if training for the first time "
      ]
    },
    {
      "metadata": {
        "id": "5kPwf9UwfGj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2473
        },
        "outputId": "726d9b3d-92bc-43a5-c356-7d581f348db6"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 10\n",
        "loss='binary_crossentropy'\n",
        "optimizer='adam'\n",
        "restore =  1 #set 0 if training for first time\n",
        "shuffle = True\n",
        "valid_ratio = 0.2\n",
        "max_vocab_size = 20000\n",
        "max_len_sentence = 200 \n",
        "glove_path  = './glove.6B.300d.txt'\n",
        "embedding_dim = 300\n",
        "\n",
        "Xtrain,Ytrain,Xval,Yval,classes = prepareData(restore, shuffle,valid_ratio)\n",
        "\n",
        "tokenizer,vocab_size = createTokens(restore,max_vocab_size,Xtrain)\n",
        "embedding_matrix = EmbeddingMatrix(tokenizer, glove_path , embedding_dim)\n",
        "Xtrain =   tokenToSequence(Xtrain,tokenizer,max_len_sentence)\n",
        "Xval   =   tokenToSequence(Xval,tokenizer,max_len_sentence)\n",
        "\n",
        "model = trainModel(Xtrain,Ytrain,Xval,Yval,epochs,batch_size,loss,optimizer,\n",
        "                   restore,vocab_size, embedding_matrix,max_len_sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded 'dftrain.csv' from Drive\n",
            "Downloaded 'dfval.csv' from Drive\n",
            "Downloaded 'tokenizer.pkl' from Drive\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            " 42944/127656 [=========>....................] - ETA: 45s - loss: 0.0120"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 66s 519us/step - loss: 0.0127\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.01268, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 27424/127656 [=====>........................] - ETA: 38s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 49s 381us/step\n",
            "31915/31915 [==============================] - 12s 381us/step\n",
            "Training Score : 0.9995219720880097 - Validation Score:0.9743328986301636\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            " 11584/127656 [=>............................] - ETA: 1:04 - loss: 0.0102"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 66s 519us/step - loss: 0.0120\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.01200, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 15456/127656 [==>...........................] - ETA: 44s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 50s 389us/step\n",
            "31915/31915 [==============================] - 12s 382us/step\n",
            "Training Score : 0.9996239603067583 - Validation Score:0.9750385821602201\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            "  9536/127656 [=>............................] - ETA: 1:07 - loss: 0.0092"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 67s 522us/step - loss: 0.0114\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.01143, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 14752/127656 [==>...........................] - ETA: 45s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 50s 388us/step\n",
            "31915/31915 [==============================] - 12s 387us/step\n",
            "Training Score : 0.9996200353931622 - Validation Score:0.9738893293048836\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            "  9216/127656 [=>............................] - ETA: 1:08 - loss: 0.0107"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 67s 522us/step - loss: 0.0110\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.01103, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 14080/127656 [==>...........................] - ETA: 46s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 51s 398us/step\n",
            "31915/31915 [==============================] - 12s 390us/step\n",
            "Training Score : 0.9996437930436661 - Validation Score:0.9751721157108482\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            "  8768/127656 [=>............................] - ETA: 1:08 - loss: 0.0102"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 67s 522us/step - loss: 0.0105\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.01055, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 15392/127656 [==>...........................] - ETA: 45s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 49s 386us/step\n",
            "31915/31915 [==============================] - 12s 386us/step\n",
            "Training Score : 0.9997060442276028 - Validation Score:0.97440119164507\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            " 10048/127656 [=>............................] - ETA: 1:07 - loss: 0.0089"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 67s 523us/step - loss: 0.0102\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.01021, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 13696/127656 [==>...........................] - ETA: 49s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 50s 395us/step\n",
            "31915/31915 [==============================] - 13s 393us/step\n",
            "Training Score : 0.999719657140428 - Validation Score:0.9743795230592864\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            "  9024/127656 [=>............................] - ETA: 1:10 - loss: 0.0096"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 67s 524us/step - loss: 0.0100\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.01003, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 14688/127656 [==>...........................] - ETA: 45s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 50s 389us/step\n",
            "31915/31915 [==============================] - 12s 384us/step\n",
            "Training Score : 0.9997763521165157 - Validation Score:0.9743190617960958\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            "  9536/127656 [=>............................] - ETA: 1:10 - loss: 0.0085"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 67s 525us/step - loss: 0.0095\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.00952, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 14496/127656 [==>...........................] - ETA: 46s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 50s 392us/step\n",
            "31915/31915 [==============================] - 13s 393us/step\n",
            "Training Score : 0.9997402334892515 - Validation Score:0.9738869596120708\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            "  9024/127656 [=>............................] - ETA: 1:12 - loss: 0.0095"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 67s 526us/step - loss: 0.0094\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.00936, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 13984/127656 [==>...........................] - ETA: 46s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 50s 390us/step\n",
            "31915/31915 [==============================] - 12s 391us/step\n",
            "Training Score : 0.9998150315530405 - Validation Score:0.9745826791506925\n",
            "Downloaded 'model.json' from Drive\n",
            "Downloaded 'weights.hdf5' from Drive\n",
            "Loaded saved model\n",
            "Epoch 1/1\n",
            "  8768/127656 [=>............................] - ETA: 1:13 - loss: 0.0071"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 67s 525us/step - loss: 0.0088\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.00877, saving model to weights.hdf5\n",
            "Saved 'model.json' to Drive\n",
            "Saved 'weights.hdf5' to Drive\n",
            " 14304/127656 [==>...........................] - ETA: 46s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127656/127656 [==============================] - 50s 391us/step\n",
            "31915/31915 [==============================] - 12s 387us/step\n",
            "Training Score : 0.9998063156635374 - Validation Score:0.9736328064248368\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Embedding (Embedding)        (None, None, 300)         54916200  \n",
            "_________________________________________________________________\n",
            "lstm (CuDNNGRU)              (None, 75)                84825     \n",
            "_________________________________________________________________\n",
            "Dense1 (Dense)               (None, 100)               7600      \n",
            "_________________________________________________________________\n",
            "Dropout1 (Dropout)           (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "Dense2 (Dense)               (None, 6)                 606       \n",
            "=================================================================\n",
            "Total params: 55,009,231\n",
            "Trainable params: 93,031\n",
            "Non-trainable params: 54,916,200\n",
            "_________________________________________________________________\n",
            "Model is evaluated on metric ROC AUC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YVYFwHKSdmMX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test the model"
      ]
    },
    {
      "metadata": {
        "id": "hBaYO15AAWil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "962ab08a-fe0e-411f-f339-e7008b4e2626"
      },
      "cell_type": "code",
      "source": [
        "Drive().fileaction(['test.csv'],'down')\n",
        "dtest = pd.read_csv('test.csv')\n",
        "Xtest = dtest['comment_text']\n",
        "dtest.info()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded 'test.csv' from Drive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 153164 entries, 0 to 153163\n",
            "Data columns (total 2 columns):\n",
            "id              153164 non-null object\n",
            "comment_text    153164 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 2.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gxYDlnpUAq30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "Xtest = tokenizer.texts_to_sequences(Xtest)\n",
        "Xtest = pad_sequences(Xtest,maxlen=max_len_sentence) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q3dunceBA3vq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6b0d18ac-4c92-45bc-dd04-a3fd9012e11b"
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict(Xtest,verbose=1)\n",
        "preds[preds>=0.5] = 1\n",
        "preds[preds<0.5] = 0\n",
        "preds.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "153164/153164 [==============================] - 60s 390us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(153164, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "_j-6HvukBKec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result = pd.DataFrame(preds,columns = classes)\n",
        "result['id'] = dtest['id']\n",
        "result.to_csv('result.csv',index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLTvPVr4B5bB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "39be153e-e61c-4e97-8373-fd44b19d2bc9"
      },
      "cell_type": "code",
      "source": [
        "result.info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 153164 entries, 0 to 153163\n",
            "Data columns (total 7 columns):\n",
            "toxic            153164 non-null float32\n",
            "severe_toxic     153164 non-null float32\n",
            "obscene          153164 non-null float32\n",
            "threat           153164 non-null float32\n",
            "insult           153164 non-null float32\n",
            "identity_hate    153164 non-null float32\n",
            "id               153164 non-null object\n",
            "dtypes: float32(6), object(1)\n",
            "memory usage: 4.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u2rp4LB5DfqR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('result.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XdlY6luuDrFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}